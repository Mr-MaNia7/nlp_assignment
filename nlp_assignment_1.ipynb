{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**ABDULKARIM GETACHEW MOHAMMED\n",
        "UGR/7992/12\n",
        "AI STREAM**\n"
      ],
      "metadata": {
        "id": "eiuWSpKsW9_n"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "VCgCCKTNSzIt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f1058509-c887-4d6f-a3cb-3778c0e93cc5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define constant variables\n"
      ],
      "metadata": {
        "id": "0fTZNpeBSsbz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "NUM_OF_CHARS = 10000000\n",
        "file_path = '/content/drive/My Drive/GPAC.txt'"
      ],
      "metadata": {
        "id": "pSVMtWlnWk58"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Whole file\n",
        "with open(file_path, 'r') as file:\n",
        "    text_full = file.read()\n"
      ],
      "metadata": {
        "id": "6BHsZS6uSqzB"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 500 bytes sample text\n",
        "with open(file_path, 'r') as file:\n",
        "    text_sample = file.read(500)\n"
      ],
      "metadata": {
        "id": "kWjLvvoESvLC"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = text_full[:NUM_OF_CHARS]\n",
        "words = text.split()\n",
        "words_sample = text_sample.split()\n"
      ],
      "metadata": {
        "id": "vQSudogXTE-J"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# #1 N-gram language model (6%)"
      ],
      "metadata": {
        "id": "6ydSuM28JiHg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate n-grams\n",
        "def generate_ngrams(given_words, n):\n",
        "    return [' '.join(given_words[i:i + n]) for i in range(len(given_words) - n + 1)]\n"
      ],
      "metadata": {
        "id": "MnbET70FZ2uw"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "\n",
        "def calculate_ngram_probabilities(ngrams):\n",
        "    ngram_counts = Counter(ngrams)\n",
        "    total_ngrams = len(ngrams)\n",
        "    return {k: v / total_ngrams for k, v in ngram_counts.items()}\n"
      ],
      "metadata": {
        "id": "oNaqL7GHfYn_"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.1 Create n-grams for n=1, 2, 3, 4. You can show sample prints.\n",
        "\n",
        "```\n",
        "Sample prints 500 bytes of data\n",
        "```\n"
      ],
      "metadata": {
        "id": "X-TQs1s3JeUk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate and print n-grams for n = 1,2,3,4\n",
        "for n in [1, 2, 3, 4]:\n",
        "    ngrams = generate_ngrams(words_sample, n)\n",
        "    print(f'{n}-grams:')\n",
        "    for ngram in ngrams:\n",
        "        print(ngram)\n",
        "    print()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u8nFFq5DdPqY",
        "outputId": "58ec2b36-9051-4dfb-f3c2-8043cb6d7ebc"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1-grams:\n",
            "ምን\n",
            "መሰላችሁ?\n",
            "(አንባቢያን)\n",
            "ኢትዮጵያ\n",
            "በተደጋጋሚ\n",
            "ጥሪው\n",
            "ደርሷት\n",
            "ልትታደመው\n",
            "ያልቻለችው\n",
            "የአለም\n",
            "የእግር\n",
            "ኳስ\n",
            "ዋ\n",
            "ለ19ኛ\n",
            "ጊዜ\n",
            "በደቡብ\n",
            "አፍሪካ\n",
            "ሲጠጣ፣\n",
            "በሩቅ\n",
            "እያየች\n",
            "አንጀቷ\n",
            "ባረረ\n",
            "ልክ\n",
            "በአመቱ\n",
            "በለስ\n",
            "ቀናትና\n",
            "ሌላ\n",
            "ዋ\n",
            "ልትታደም\n",
            "ሁለት\n",
            "ልጆቿን\n",
            "ወደ\n",
            "ደቡብ\n",
            "አፍሪካ\n",
            "ላከች፡፡6ኛው\n",
            "ቢግ\n",
            "ብራዘርስ\n",
            "አፍሪካ\n",
            "አብሮ\n",
            "የመኖር\n",
            "ውድድር\n",
            "በደቡብ\n",
            "አፍሪካ\n",
            "ተካሂዷል፡፡\n",
            "ከተለያዩ\n",
            "14\n",
            "የአፍሪካ\n",
            "አገራት\n",
            "የተውጣጡ\n",
            "26\n",
            "ያህል\n",
            "ተሳታፊዎች\n",
            "የተካፈሉበት\n",
            "ይህ\n",
            "ውድድር፣\n",
            "ግለሰቦች\n",
            "በፈታኝ\n",
            "ሁኔታ\n",
            "ውስጥ\n",
            "በማለፍ\n",
            "ብቃታቸውን\n",
            "የሚያስመሰክሩበት\n",
            "መሆኑን\n",
            "ሰምተናል፡፡\n",
            "የሚገጥሟቸውን\n",
            "የተለያዩ\n",
            "ፈተናዎች\n",
            "በትእግስትና\n",
            "በጥበብ\n",
            "ማለፍ፣\n",
            "ከሌሎች\n",
            "ጋር\n",
            "ተስማምቶ\n",
            "መዝለቅ፣\n",
            "ችግሮችን\n",
            "በብልጠት\n",
            "መፍታት\n",
            "ወዘተ\n",
            "በየጊዜው\n",
            "ከሚደረገው\n",
            "ቅነሳ\n",
            "ተርፈው\n",
            "ለ91\n",
            "ቀናት\n",
            "ያህል\n",
            "በውድድሩ\n",
            "መቆየት\n",
            "የቻሉ\n",
            "ሁለት\n",
            "ተወዳዳሪዎች\n",
            "እያንዳንዳቸው\n",
            "200\n",
            "ሺህ\n",
            "ዶላር\n",
            "እንደሚ\n",
            "\n",
            "2-grams:\n",
            "ምን መሰላችሁ?\n",
            "መሰላችሁ? (አንባቢያን)\n",
            "(አንባቢያን) ኢትዮጵያ\n",
            "ኢትዮጵያ በተደጋጋሚ\n",
            "በተደጋጋሚ ጥሪው\n",
            "ጥሪው ደርሷት\n",
            "ደርሷት ልትታደመው\n",
            "ልትታደመው ያልቻለችው\n",
            "ያልቻለችው የአለም\n",
            "የአለም የእግር\n",
            "የእግር ኳስ\n",
            "ኳስ ዋ\n",
            "ዋ ለ19ኛ\n",
            "ለ19ኛ ጊዜ\n",
            "ጊዜ በደቡብ\n",
            "በደቡብ አፍሪካ\n",
            "አፍሪካ ሲጠጣ፣\n",
            "ሲጠጣ፣ በሩቅ\n",
            "በሩቅ እያየች\n",
            "እያየች አንጀቷ\n",
            "አንጀቷ ባረረ\n",
            "ባረረ ልክ\n",
            "ልክ በአመቱ\n",
            "በአመቱ በለስ\n",
            "በለስ ቀናትና\n",
            "ቀናትና ሌላ\n",
            "ሌላ ዋ\n",
            "ዋ ልትታደም\n",
            "ልትታደም ሁለት\n",
            "ሁለት ልጆቿን\n",
            "ልጆቿን ወደ\n",
            "ወደ ደቡብ\n",
            "ደቡብ አፍሪካ\n",
            "አፍሪካ ላከች፡፡6ኛው\n",
            "ላከች፡፡6ኛው ቢግ\n",
            "ቢግ ብራዘርስ\n",
            "ብራዘርስ አፍሪካ\n",
            "አፍሪካ አብሮ\n",
            "አብሮ የመኖር\n",
            "የመኖር ውድድር\n",
            "ውድድር በደቡብ\n",
            "በደቡብ አፍሪካ\n",
            "አፍሪካ ተካሂዷል፡፡\n",
            "ተካሂዷል፡፡ ከተለያዩ\n",
            "ከተለያዩ 14\n",
            "14 የአፍሪካ\n",
            "የአፍሪካ አገራት\n",
            "አገራት የተውጣጡ\n",
            "የተውጣጡ 26\n",
            "26 ያህል\n",
            "ያህል ተሳታፊዎች\n",
            "ተሳታፊዎች የተካፈሉበት\n",
            "የተካፈሉበት ይህ\n",
            "ይህ ውድድር፣\n",
            "ውድድር፣ ግለሰቦች\n",
            "ግለሰቦች በፈታኝ\n",
            "በፈታኝ ሁኔታ\n",
            "ሁኔታ ውስጥ\n",
            "ውስጥ በማለፍ\n",
            "በማለፍ ብቃታቸውን\n",
            "ብቃታቸውን የሚያስመሰክሩበት\n",
            "የሚያስመሰክሩበት መሆኑን\n",
            "መሆኑን ሰምተናል፡፡\n",
            "ሰምተናል፡፡ የሚገጥሟቸውን\n",
            "የሚገጥሟቸውን የተለያዩ\n",
            "የተለያዩ ፈተናዎች\n",
            "ፈተናዎች በትእግስትና\n",
            "በትእግስትና በጥበብ\n",
            "በጥበብ ማለፍ፣\n",
            "ማለፍ፣ ከሌሎች\n",
            "ከሌሎች ጋር\n",
            "ጋር ተስማምቶ\n",
            "ተስማምቶ መዝለቅ፣\n",
            "መዝለቅ፣ ችግሮችን\n",
            "ችግሮችን በብልጠት\n",
            "በብልጠት መፍታት\n",
            "መፍታት ወዘተ\n",
            "ወዘተ በየጊዜው\n",
            "በየጊዜው ከሚደረገው\n",
            "ከሚደረገው ቅነሳ\n",
            "ቅነሳ ተርፈው\n",
            "ተርፈው ለ91\n",
            "ለ91 ቀናት\n",
            "ቀናት ያህል\n",
            "ያህል በውድድሩ\n",
            "በውድድሩ መቆየት\n",
            "መቆየት የቻሉ\n",
            "የቻሉ ሁለት\n",
            "ሁለት ተወዳዳሪዎች\n",
            "ተወዳዳሪዎች እያንዳንዳቸው\n",
            "እያንዳንዳቸው 200\n",
            "200 ሺህ\n",
            "ሺህ ዶላር\n",
            "ዶላር እንደሚ\n",
            "\n",
            "3-grams:\n",
            "ምን መሰላችሁ? (አንባቢያን)\n",
            "መሰላችሁ? (አንባቢያን) ኢትዮጵያ\n",
            "(አንባቢያን) ኢትዮጵያ በተደጋጋሚ\n",
            "ኢትዮጵያ በተደጋጋሚ ጥሪው\n",
            "በተደጋጋሚ ጥሪው ደርሷት\n",
            "ጥሪው ደርሷት ልትታደመው\n",
            "ደርሷት ልትታደመው ያልቻለችው\n",
            "ልትታደመው ያልቻለችው የአለም\n",
            "ያልቻለችው የአለም የእግር\n",
            "የአለም የእግር ኳስ\n",
            "የእግር ኳስ ዋ\n",
            "ኳስ ዋ ለ19ኛ\n",
            "ዋ ለ19ኛ ጊዜ\n",
            "ለ19ኛ ጊዜ በደቡብ\n",
            "ጊዜ በደቡብ አፍሪካ\n",
            "በደቡብ አፍሪካ ሲጠጣ፣\n",
            "አፍሪካ ሲጠጣ፣ በሩቅ\n",
            "ሲጠጣ፣ በሩቅ እያየች\n",
            "በሩቅ እያየች አንጀቷ\n",
            "እያየች አንጀቷ ባረረ\n",
            "አንጀቷ ባረረ ልክ\n",
            "ባረረ ልክ በአመቱ\n",
            "ልክ በአመቱ በለስ\n",
            "በአመቱ በለስ ቀናትና\n",
            "በለስ ቀናትና ሌላ\n",
            "ቀናትና ሌላ ዋ\n",
            "ሌላ ዋ ልትታደም\n",
            "ዋ ልትታደም ሁለት\n",
            "ልትታደም ሁለት ልጆቿን\n",
            "ሁለት ልጆቿን ወደ\n",
            "ልጆቿን ወደ ደቡብ\n",
            "ወደ ደቡብ አፍሪካ\n",
            "ደቡብ አፍሪካ ላከች፡፡6ኛው\n",
            "አፍሪካ ላከች፡፡6ኛው ቢግ\n",
            "ላከች፡፡6ኛው ቢግ ብራዘርስ\n",
            "ቢግ ብራዘርስ አፍሪካ\n",
            "ብራዘርስ አፍሪካ አብሮ\n",
            "አፍሪካ አብሮ የመኖር\n",
            "አብሮ የመኖር ውድድር\n",
            "የመኖር ውድድር በደቡብ\n",
            "ውድድር በደቡብ አፍሪካ\n",
            "በደቡብ አፍሪካ ተካሂዷል፡፡\n",
            "አፍሪካ ተካሂዷል፡፡ ከተለያዩ\n",
            "ተካሂዷል፡፡ ከተለያዩ 14\n",
            "ከተለያዩ 14 የአፍሪካ\n",
            "14 የአፍሪካ አገራት\n",
            "የአፍሪካ አገራት የተውጣጡ\n",
            "አገራት የተውጣጡ 26\n",
            "የተውጣጡ 26 ያህል\n",
            "26 ያህል ተሳታፊዎች\n",
            "ያህል ተሳታፊዎች የተካፈሉበት\n",
            "ተሳታፊዎች የተካፈሉበት ይህ\n",
            "የተካፈሉበት ይህ ውድድር፣\n",
            "ይህ ውድድር፣ ግለሰቦች\n",
            "ውድድር፣ ግለሰቦች በፈታኝ\n",
            "ግለሰቦች በፈታኝ ሁኔታ\n",
            "በፈታኝ ሁኔታ ውስጥ\n",
            "ሁኔታ ውስጥ በማለፍ\n",
            "ውስጥ በማለፍ ብቃታቸውን\n",
            "በማለፍ ብቃታቸውን የሚያስመሰክሩበት\n",
            "ብቃታቸውን የሚያስመሰክሩበት መሆኑን\n",
            "የሚያስመሰክሩበት መሆኑን ሰምተናል፡፡\n",
            "መሆኑን ሰምተናል፡፡ የሚገጥሟቸውን\n",
            "ሰምተናል፡፡ የሚገጥሟቸውን የተለያዩ\n",
            "የሚገጥሟቸውን የተለያዩ ፈተናዎች\n",
            "የተለያዩ ፈተናዎች በትእግስትና\n",
            "ፈተናዎች በትእግስትና በጥበብ\n",
            "በትእግስትና በጥበብ ማለፍ፣\n",
            "በጥበብ ማለፍ፣ ከሌሎች\n",
            "ማለፍ፣ ከሌሎች ጋር\n",
            "ከሌሎች ጋር ተስማምቶ\n",
            "ጋር ተስማምቶ መዝለቅ፣\n",
            "ተስማምቶ መዝለቅ፣ ችግሮችን\n",
            "መዝለቅ፣ ችግሮችን በብልጠት\n",
            "ችግሮችን በብልጠት መፍታት\n",
            "በብልጠት መፍታት ወዘተ\n",
            "መፍታት ወዘተ በየጊዜው\n",
            "ወዘተ በየጊዜው ከሚደረገው\n",
            "በየጊዜው ከሚደረገው ቅነሳ\n",
            "ከሚደረገው ቅነሳ ተርፈው\n",
            "ቅነሳ ተርፈው ለ91\n",
            "ተርፈው ለ91 ቀናት\n",
            "ለ91 ቀናት ያህል\n",
            "ቀናት ያህል በውድድሩ\n",
            "ያህል በውድድሩ መቆየት\n",
            "በውድድሩ መቆየት የቻሉ\n",
            "መቆየት የቻሉ ሁለት\n",
            "የቻሉ ሁለት ተወዳዳሪዎች\n",
            "ሁለት ተወዳዳሪዎች እያንዳንዳቸው\n",
            "ተወዳዳሪዎች እያንዳንዳቸው 200\n",
            "እያንዳንዳቸው 200 ሺህ\n",
            "200 ሺህ ዶላር\n",
            "ሺህ ዶላር እንደሚ\n",
            "\n",
            "4-grams:\n",
            "ምን መሰላችሁ? (አንባቢያን) ኢትዮጵያ\n",
            "መሰላችሁ? (አንባቢያን) ኢትዮጵያ በተደጋጋሚ\n",
            "(አንባቢያን) ኢትዮጵያ በተደጋጋሚ ጥሪው\n",
            "ኢትዮጵያ በተደጋጋሚ ጥሪው ደርሷት\n",
            "በተደጋጋሚ ጥሪው ደርሷት ልትታደመው\n",
            "ጥሪው ደርሷት ልትታደመው ያልቻለችው\n",
            "ደርሷት ልትታደመው ያልቻለችው የአለም\n",
            "ልትታደመው ያልቻለችው የአለም የእግር\n",
            "ያልቻለችው የአለም የእግር ኳስ\n",
            "የአለም የእግር ኳስ ዋ\n",
            "የእግር ኳስ ዋ ለ19ኛ\n",
            "ኳስ ዋ ለ19ኛ ጊዜ\n",
            "ዋ ለ19ኛ ጊዜ በደቡብ\n",
            "ለ19ኛ ጊዜ በደቡብ አፍሪካ\n",
            "ጊዜ በደቡብ አፍሪካ ሲጠጣ፣\n",
            "በደቡብ አፍሪካ ሲጠጣ፣ በሩቅ\n",
            "አፍሪካ ሲጠጣ፣ በሩቅ እያየች\n",
            "ሲጠጣ፣ በሩቅ እያየች አንጀቷ\n",
            "በሩቅ እያየች አንጀቷ ባረረ\n",
            "እያየች አንጀቷ ባረረ ልክ\n",
            "አንጀቷ ባረረ ልክ በአመቱ\n",
            "ባረረ ልክ በአመቱ በለስ\n",
            "ልክ በአመቱ በለስ ቀናትና\n",
            "በአመቱ በለስ ቀናትና ሌላ\n",
            "በለስ ቀናትና ሌላ ዋ\n",
            "ቀናትና ሌላ ዋ ልትታደም\n",
            "ሌላ ዋ ልትታደም ሁለት\n",
            "ዋ ልትታደም ሁለት ልጆቿን\n",
            "ልትታደም ሁለት ልጆቿን ወደ\n",
            "ሁለት ልጆቿን ወደ ደቡብ\n",
            "ልጆቿን ወደ ደቡብ አፍሪካ\n",
            "ወደ ደቡብ አፍሪካ ላከች፡፡6ኛው\n",
            "ደቡብ አፍሪካ ላከች፡፡6ኛው ቢግ\n",
            "አፍሪካ ላከች፡፡6ኛው ቢግ ብራዘርስ\n",
            "ላከች፡፡6ኛው ቢግ ብራዘርስ አፍሪካ\n",
            "ቢግ ብራዘርስ አፍሪካ አብሮ\n",
            "ብራዘርስ አፍሪካ አብሮ የመኖር\n",
            "አፍሪካ አብሮ የመኖር ውድድር\n",
            "አብሮ የመኖር ውድድር በደቡብ\n",
            "የመኖር ውድድር በደቡብ አፍሪካ\n",
            "ውድድር በደቡብ አፍሪካ ተካሂዷል፡፡\n",
            "በደቡብ አፍሪካ ተካሂዷል፡፡ ከተለያዩ\n",
            "አፍሪካ ተካሂዷል፡፡ ከተለያዩ 14\n",
            "ተካሂዷል፡፡ ከተለያዩ 14 የአፍሪካ\n",
            "ከተለያዩ 14 የአፍሪካ አገራት\n",
            "14 የአፍሪካ አገራት የተውጣጡ\n",
            "የአፍሪካ አገራት የተውጣጡ 26\n",
            "አገራት የተውጣጡ 26 ያህል\n",
            "የተውጣጡ 26 ያህል ተሳታፊዎች\n",
            "26 ያህል ተሳታፊዎች የተካፈሉበት\n",
            "ያህል ተሳታፊዎች የተካፈሉበት ይህ\n",
            "ተሳታፊዎች የተካፈሉበት ይህ ውድድር፣\n",
            "የተካፈሉበት ይህ ውድድር፣ ግለሰቦች\n",
            "ይህ ውድድር፣ ግለሰቦች በፈታኝ\n",
            "ውድድር፣ ግለሰቦች በፈታኝ ሁኔታ\n",
            "ግለሰቦች በፈታኝ ሁኔታ ውስጥ\n",
            "በፈታኝ ሁኔታ ውስጥ በማለፍ\n",
            "ሁኔታ ውስጥ በማለፍ ብቃታቸውን\n",
            "ውስጥ በማለፍ ብቃታቸውን የሚያስመሰክሩበት\n",
            "በማለፍ ብቃታቸውን የሚያስመሰክሩበት መሆኑን\n",
            "ብቃታቸውን የሚያስመሰክሩበት መሆኑን ሰምተናል፡፡\n",
            "የሚያስመሰክሩበት መሆኑን ሰምተናል፡፡ የሚገጥሟቸውን\n",
            "መሆኑን ሰምተናል፡፡ የሚገጥሟቸውን የተለያዩ\n",
            "ሰምተናል፡፡ የሚገጥሟቸውን የተለያዩ ፈተናዎች\n",
            "የሚገጥሟቸውን የተለያዩ ፈተናዎች በትእግስትና\n",
            "የተለያዩ ፈተናዎች በትእግስትና በጥበብ\n",
            "ፈተናዎች በትእግስትና በጥበብ ማለፍ፣\n",
            "በትእግስትና በጥበብ ማለፍ፣ ከሌሎች\n",
            "በጥበብ ማለፍ፣ ከሌሎች ጋር\n",
            "ማለፍ፣ ከሌሎች ጋር ተስማምቶ\n",
            "ከሌሎች ጋር ተስማምቶ መዝለቅ፣\n",
            "ጋር ተስማምቶ መዝለቅ፣ ችግሮችን\n",
            "ተስማምቶ መዝለቅ፣ ችግሮችን በብልጠት\n",
            "መዝለቅ፣ ችግሮችን በብልጠት መፍታት\n",
            "ችግሮችን በብልጠት መፍታት ወዘተ\n",
            "በብልጠት መፍታት ወዘተ በየጊዜው\n",
            "መፍታት ወዘተ በየጊዜው ከሚደረገው\n",
            "ወዘተ በየጊዜው ከሚደረገው ቅነሳ\n",
            "በየጊዜው ከሚደረገው ቅነሳ ተርፈው\n",
            "ከሚደረገው ቅነሳ ተርፈው ለ91\n",
            "ቅነሳ ተርፈው ለ91 ቀናት\n",
            "ተርፈው ለ91 ቀናት ያህል\n",
            "ለ91 ቀናት ያህል በውድድሩ\n",
            "ቀናት ያህል በውድድሩ መቆየት\n",
            "ያህል በውድድሩ መቆየት የቻሉ\n",
            "በውድድሩ መቆየት የቻሉ ሁለት\n",
            "መቆየት የቻሉ ሁለት ተወዳዳሪዎች\n",
            "የቻሉ ሁለት ተወዳዳሪዎች እያንዳንዳቸው\n",
            "ሁለት ተወዳዳሪዎች እያንዳንዳቸው 200\n",
            "ተወዳዳሪዎች እያንዳንዳቸው 200 ሺህ\n",
            "እያንዳንዳቸው 200 ሺህ ዶላር\n",
            "200 ሺህ ዶላር እንደሚ\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.2 Calculate probabilities of n-grams and find the top 10 most likely n-grams for all n.\n"
      ],
      "metadata": {
        "id": "QhRLIBM7LFrS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate and print n-grams for different values of n\n",
        "for n in [1, 2, 3, 4]:\n",
        "    ngrams = generate_ngrams(words, n)\n",
        "\n",
        "    # Calculate probabilities\n",
        "    probabilities = calculate_ngram_probabilities(ngrams)\n",
        "\n",
        "    # Find the top 10 most likely n-grams\n",
        "    top_10_ngrams = sorted(probabilities.items(), key=lambda x: x[1], reverse=True)[:10]\n",
        "\n",
        "    # Print results\n",
        "    print(f'{n}-grams:')\n",
        "    print(\"Top 10 most likely:\")\n",
        "    print()\n",
        "    for ngram, probability in top_10_ngrams:\n",
        "        print(f\"{ngram}: {probability:.4f}\")\n",
        "\n",
        "    print()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o9XozYDVgaHt",
        "outputId": "e12d7e5e-0be1-412e-f346-4c64635dbb42"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1-grams:\n",
            "Top 10 most likely:\n",
            "\n",
            "ላይ: 0.0088\n",
            "ነው፡፡: 0.0079\n",
            "ነው: 0.0066\n",
            "ግን: 0.0046\n",
            "ወደ: 0.0046\n",
            "ውስጥ: 0.0045\n",
            "እና: 0.0043\n",
            "ነገር: 0.0037\n",
            "ጋር: 0.0036\n",
            "ጊዜ: 0.0031\n",
            "\n",
            "2-grams:\n",
            "Top 10 most likely:\n",
            "\n",
            "ዓ ም: 0.0011\n",
            "( ): 0.0009\n",
            "ነገር ግን: 0.0006\n",
            "ብቻ ሳይሆን: 0.0004\n",
            "አዲስ አበባ: 0.0004\n",
            "ማለት ነው፡፡: 0.0003\n",
            "ምን ያህል: 0.0003\n",
            "በአዲስ አበባ: 0.0002\n",
            "ሚሊዮን ዶላር: 0.0002\n",
            "ብቻ ነው፡፡: 0.0002\n",
            "\n",
            "3-grams:\n",
            "Top 10 most likely:\n",
            "\n",
            "እ ኤ አ: 0.0002\n",
            "2004 ዓ ም: 0.0001\n",
            "ቀን 2004 ዓ: 0.0001\n",
            "ወደ አዲስ አበባ: 0.0001\n",
            "2005 ዓ ም: 0.0001\n",
            "ጠቅላይ ሚኒስትር መለስ: 0.0001\n",
            "ዓ ም ጀምሮ: 0.0000\n",
            "የጽንስና ማህጸን ሐኪሞች: 0.0000\n",
            "ቀን 2005 ዓ: 0.0000\n",
            "ቂ ቂ ቂ: 0.0000\n",
            "\n",
            "4-grams:\n",
            "Top 10 most likely:\n",
            "\n",
            "ቀን 2004 ዓ ም: 0.0001\n",
            "ቀን 2005 ዓ ም: 0.0000\n",
            "ጨዋታን ጨዋታ ያነሳው የለ: 0.0000\n",
            "የጽንስና ማህጸን ሐኪሞች ማህበር: 0.0000\n",
            "እንግዲህ ጨዋታን ጨዋታ ያነሳው: 0.0000\n",
            "የኢትዮጵያ የጽንስና ማህጸን ሐኪሞች: 0.0000\n",
            "ጠቅላይ ሚኒስትር መለስ ዜናዊ: 0.0000\n",
            "ቀን 2003 ዓ ም: 0.0000\n",
            "በብርሃንና ሰላም ማተሚያ ቤት: 0.0000\n",
            "የጽንስና ማህጸን ሕክምና እስፔሻሊስት: 0.0000\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.3 What is the probability of the sentence. \"ኢትዮጵያ ታሪካዊ ሀገር ናት \". You can also try more sentences.\n"
      ],
      "metadata": {
        "id": "mszRqoukpBaw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_sentence_probability(sentence, ngram_probabilities):\n",
        "    sentence_ngrams = generate_ngrams(sentence.split(), len(list(ngram_probabilities.keys())[0].split()))\n",
        "    sentence_probability = 1.0\n",
        "    for ngram in sentence_ngrams:\n",
        "        sentence_probability *= ngram_probabilities.get(ngram, 0.0)\n",
        "    return sentence_probability\n",
        "\n",
        "# Calculate n-gram probabilities for a sample\n",
        "ngrams = generate_ngrams(words, 4)\n",
        "ngram_probabilities = calculate_ngram_probabilities(ngrams)\n",
        "\n",
        "sentences = [\"ኢትዮጵያ ታሪካዊ ሀገር ናት\", \"ኢትዮጵያ በተደጋጋሚ ጥሪው ደርሷት\", \"ልትታደመው ያልቻለችው የአለም የእግር ኳስ\", \"በአመቱ በለስ ቀናትና ሌላ\", \"ለእናንተም መልካም ቀን\"]\n",
        "\n",
        "# Calculate the probability of the sentences\n",
        "for sentence in sentences:\n",
        "  sentence_probability = calculate_sentence_probability(sentence, ngram_probabilities)\n",
        "  print(f\"The probability of the sentence '{sentence}' is: {sentence_probability:.10f}\")\n",
        "print()\n"
      ],
      "metadata": {
        "id": "EqkHuiODV5ce",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "50dbe751-b1de-495a-8789-0c8f1a88ed79"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The probability of the sentence 'ኢትዮጵያ ታሪካዊ ሀገር ናት' is: 0.0000000000\n",
            "The probability of the sentence 'ኢትዮጵያ በተደጋጋሚ ጥሪው ደርሷት' is: 0.0000005345\n",
            "The probability of the sentence 'ልትታደመው ያልቻለችው የአለም የእግር ኳስ' is: 0.0000000000\n",
            "The probability of the sentence 'በአመቱ በለስ ቀናትና ሌላ' is: 0.0000005345\n",
            "The probability of the sentence 'ለእናንተም መልካም ቀን' is: 1.0000000000\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.4 Generate random sentences using n-grams; explain what happens as n increases, based on your output.\n"
      ],
      "metadata": {
        "id": "tRUAp5GWpECv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "def generate_random_sentence(ngram_probabilities, max_length=20):\n",
        "    sentence = []\n",
        "    current_ngram = random.choice(list(ngram_probabilities.keys()))\n",
        "\n",
        "    while len(sentence) < max_length:\n",
        "        sentence.extend(current_ngram.split())\n",
        "        next_word = random.choices(\n",
        "            list(ngram_probabilities.keys()),\n",
        "            weights=list(ngram_probabilities.values())\n",
        "        )[0].split()[-1]\n",
        "        current_ngram = ' '.join(current_ngram.split()[1:] + [next_word])\n",
        "\n",
        "    return ' '.join(sentence)\n",
        "\n",
        "# Generate random sentences for different values of n\n",
        "for n in [1, 2, 3, 4]:\n",
        "    ngrams = generate_ngrams(words, n)\n",
        "    ngram_probabilities = calculate_ngram_probabilities(ngrams)\n",
        "    print(f\"\\nGenerated random sentence for n: {n}:\")\n",
        "    random_sentence = generate_random_sentence(ngram_probabilities, 20)\n",
        "    print(random_sentence)\n"
      ],
      "metadata": {
        "id": "pkTXXxDcWa9B",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7a3f89ca-2b97-4426-a8fd-a56ed6b61ab8"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Generated random sentence for n: 1:\n",
            "የሚያገድመውን አሁንም ፍርፋሪዎች የማያውቁ ክለብ አምባገነን እነዚህ ይወስደዋል፡፡ ችግር ደረጃ ለመሸጥ የአፍሪካ እንዲህ ባለንበት በእነዚህ ብሎ ነኝ፡፡ እና የኢትዮጵያ በማወቅ\n",
            "\n",
            "Generated random sentence for n: 2:\n",
            "ማስታወስ ከቻልን ከቻልን ዝፈን ዝፈን የሚችሉ የሚችሉ እንደ እንደ ረቡእ ረቡእ ሆንኩት? ሆንኩት? ሠራተኞች ሠራተኞች ማሰቡ ማሰቡ የተሸጠውን የተሸጠውን እየተቧደናችሁ\n",
            "\n",
            "Generated random sentence for n: 3:\n",
            "አሜን! አሊያስ አዲስ አሊያስ አዲስ አለበት፡፡ አዲስ አለበት፡፡ በእነዚህ አለበት፡፡ በእነዚህ ይችላል፡፡ በእነዚህ ይችላል፡፡ ደቂቃዎች ይችላል፡፡ ደቂቃዎች የሚለውን ደቂቃዎች የሚለውን ቸር\n",
            "\n",
            "Generated random sentence for n: 4:\n",
            "ፅሁፍ (ታላቁዋን ብሪታንያን) ሲገልፅ፣ (ታላቁዋን ብሪታንያን) ሲገልፅ፣ ጣት ብሪታንያን) ሲገልፅ፣ ጣት ብራውን፤ ሲገልፅ፣ ጣት ብራውን፤ ያልጠረጠረ ጣት ብራውን፤ ያልጠረጠረ ዜብራዎቹ\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Explanation:\n",
        "\n",
        "As the value of n increases:\n",
        "\n",
        "1. **Increased Coherence:** Longer n-grams capture more context, making generated sentences more coherent.\n",
        "\n",
        "2. **Higher Repetition Risk:** Sensitivity to specific patterns in training data may lead to repetitive sequences in generated sentences.\n",
        "\n",
        "3. **Balancing Act:** Choosing an appropriate n involves a trade-off between capturing context and avoiding overfitting to training data.\n",
        "\n",
        "In summary, larger n improves coherence but increases the risk of repetition, requiring a careful balance based on the specific characteristics of the language model and training data."
      ],
      "metadata": {
        "id": "8N1H_gjwtG9L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# #2 Evaluate these Language Models Using Intrinsic Evaluation Method (4%)\n"
      ],
      "metadata": {
        "id": "lDKrkt4RqiW5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Downloading necessary packages"
      ],
      "metadata": {
        "id": "JKeVtDpuzCjM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.lm import Laplace\n",
        "from nltk.lm.preprocessing import padded_everygram_pipeline\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from nltk.lm.vocabulary import Vocabulary\n",
        "\n",
        "# Download the necessary NLTK resources\n",
        "nltk.download('punkt')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IzN5g_-Lw0W3",
        "outputId": "0c3886c2-b675-4532-f2f1-9281dc4f7f88"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Pre-Processing\n"
      ],
      "metadata": {
        "id": "_eYu2GshypHR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize the text into sentences and words\n",
        "tokenized_text = [word_tokenize(sent) for sent in sent_tokenize(text)]\n",
        "\n",
        "# Split the data into training and validation sets\n",
        "train_data, valid_data = tokenized_text[:int(0.5 * len(tokenized_text))], tokenized_text[int(0.5 * len(tokenized_text)):]"
      ],
      "metadata": {
        "id": "p0Q32F0zyn0y"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training a Maximum Likelihood Estimation (MLE) language model\n"
      ],
      "metadata": {
        "id": "1vD3HbmBzs3c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n = 1\n",
        "train_ngrams, vocab = padded_everygram_pipeline(n, train_data)\n",
        "\n",
        "# Convert vocab to nltk.lm.Vocabulary\n",
        "vocab = Vocabulary(vocab)\n",
        "\n",
        "lm = Laplace(order=n, vocabulary=vocab)\n",
        "lm.fit(train_ngrams)\n",
        "\n",
        "print(\"Training Finished\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nw0blFcYWosn",
        "outputId": "422a0b89-9f8e-40c9-a8a5-4001b920e227"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Finished\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Intrinsic Evaluation Metrics"
      ],
      "metadata": {
        "id": "dXOZrFc90es5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Perplexity\n",
        "valid_ngrams = [list(nltk.lm.preprocessing.pad_both_ends(sent, n=n)) for sent in valid_data]\n",
        "perplexity = lm.perplexity(valid_ngrams)\n",
        "\n",
        "# 2. Grammatical Correctness\n",
        "generated_sentence = random_sentence\n",
        "generated_sentence_tokens = word_tokenize(generated_sentence)\n",
        "grammatical_correctness = all(token in vocab for token in generated_sentence_tokens)\n",
        "\n",
        "# Print Evaluation Metrics\n",
        "print(f\"Perplexity: {perplexity:.4f}\")\n",
        "print(f\"Grammatical Correctness: {grammatical_correctness}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8VB4gLzSydtH",
        "outputId": "01173069-4300-4cad-c600-2c2709723df6"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Perplexity: 193732.5579\n",
            "Grammatical Correctness: True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# #3 Evaluate these Language Models Using Extrinsic Evaluation Method (4%)"
      ],
      "metadata": {
        "id": "y1oNqL3U5AXM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Extrinsic Evaluation: Text Completion Task\n"
      ],
      "metadata": {
        "id": "zMPyNcHV45OL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def complete_text(input_text, model, n, num_generations=10):\n",
        "    completed_text = input_text\n",
        "\n",
        "    for _ in range(num_generations):\n",
        "        input_tokens = nltk.word_tokenize(completed_text)\n",
        "        input_ngrams = list(nltk.ngrams(input_tokens, n))\n",
        "\n",
        "        # Predict the next word using the language model\n",
        "        next_word = model.generate(1)\n",
        "\n",
        "        completed_text += ' ' + next_word\n",
        "\n",
        "    return completed_text\n",
        "\n",
        "input_text = \"የአፍሪካ አገራት\"\n",
        "for i in range(5):\n",
        "  completed_text = complete_text(input_text, lm, 4, num_generations=10)\n",
        "  print(f\"Completed Text: {i}\", completed_text)\n"
      ],
      "metadata": {
        "id": "EJ3ThOLwY_Lo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "40e81a23-844f-4dcd-c7a2-0a7d120218ce"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Completed Text: 0 የአፍሪካ አገራት የዳኝነት በፒስ ሄኖክ ጅብ ፈረስላይ አርሶ ደራስያን፣ ተነባቢ እጫወት ያማታል፡፡\n",
            "Completed Text: 1 የአፍሪካ አገራት አስተውያለሁ፡፡ የሁለታችንን ለተሳታፊዎችና በሰብአዊነት የአቶ ! የሚያወጣ ለዕይታ የጥናቱ የአየር\n",
            "Completed Text: 2 የአፍሪካ አገራት የአገራቸውን ለመካስ ወንበራቸው በኩል እስፐርም እንዳልጐዳ፤ ቡድን የጤና ላይ ረገድ\n",
            "Completed Text: 3 የአፍሪካ አገራት የፊት መዋጋትም እንደከረመ በርካታ መርከብ የዕድሜ እግረ ሄግል። ለድፍን ቤተሰቦቹን\n",
            "Completed Text: 4 የአፍሪካ አገራት ለሚታመሙ ኤሌክትሪክ የጨረቃ የስራ ደስ እንጂ፣ ያልነው ነው፡፡ የተነሳሁት፡፡ ሺዋን\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-m_aWW7mBOuc"
      },
      "execution_count": 30,
      "outputs": []
    }
  ]
}