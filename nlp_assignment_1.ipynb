{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**ABDULKARIM GETACHEW MOHAMMED\n",
        "UGR/7992/12\n",
        "AI STREAM**\n"
      ],
      "metadata": {
        "id": "eiuWSpKsW9_n"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "VCgCCKTNSzIt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f8f56883-52c3-4b46-d4d7-63825da8f826"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define constant variables\n"
      ],
      "metadata": {
        "id": "0fTZNpeBSsbz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "NUM_OF_CHARS = 15000000\n",
        "file_path = '/content/drive/My Drive/GPAC.txt'"
      ],
      "metadata": {
        "id": "pSVMtWlnWk58"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Whole file\n",
        "with open(file_path, 'r') as file:\n",
        "    text_full = file.read()\n"
      ],
      "metadata": {
        "id": "6BHsZS6uSqzB"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 500 bytes sample text\n",
        "with open(file_path, 'r') as file:\n",
        "    text_sample = file.read(500)\n"
      ],
      "metadata": {
        "id": "kWjLvvoESvLC"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = text_full[:NUM_OF_CHARS]\n",
        "words = text.split()\n",
        "words_sample = text_sample.split()\n"
      ],
      "metadata": {
        "id": "vQSudogXTE-J"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# #1 N-gram language model (6%)"
      ],
      "metadata": {
        "id": "6ydSuM28JiHg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate n-grams\n",
        "def generate_ngrams(given_words, n):\n",
        "    return [' '.join(given_words[i:i + n]) for i in range(len(given_words) - n + 1)]\n"
      ],
      "metadata": {
        "id": "MnbET70FZ2uw"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "\n",
        "def calculate_ngram_probabilities(ngrams):\n",
        "    ngram_counts = Counter(ngrams)\n",
        "    total_ngrams = len(ngrams)\n",
        "    return {k: v / total_ngrams for k, v in ngram_counts.items()}\n"
      ],
      "metadata": {
        "id": "oNaqL7GHfYn_"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.1 Create n-grams for n=1, 2, 3, 4. You can show sample prints.\n",
        "\n",
        "```\n",
        "Sample prints 500 bytes of data\n",
        "```\n"
      ],
      "metadata": {
        "id": "X-TQs1s3JeUk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate and print n-grams for n = 1,2,3,4\n",
        "for n in [1, 2, 3, 4]:\n",
        "    ngrams = generate_ngrams(words_sample, n)\n",
        "    print(f'{n}-grams:')\n",
        "    for ngram in ngrams:\n",
        "        print(ngram)\n",
        "    print()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u8nFFq5DdPqY",
        "outputId": "dcfdb5a2-e338-44f1-a3e6-94013aa5ef5a"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1-grams:\n",
            "ምን\n",
            "መሰላችሁ?\n",
            "(አንባቢያን)\n",
            "ኢትዮጵያ\n",
            "በተደጋጋሚ\n",
            "ጥሪው\n",
            "ደርሷት\n",
            "ልትታደመው\n",
            "ያልቻለችው\n",
            "የአለም\n",
            "የእግር\n",
            "ኳስ\n",
            "ዋ\n",
            "ለ19ኛ\n",
            "ጊዜ\n",
            "በደቡብ\n",
            "አፍሪካ\n",
            "ሲጠጣ፣\n",
            "በሩቅ\n",
            "እያየች\n",
            "አንጀቷ\n",
            "ባረረ\n",
            "ልክ\n",
            "በአመቱ\n",
            "በለስ\n",
            "ቀናትና\n",
            "ሌላ\n",
            "ዋ\n",
            "ልትታደም\n",
            "ሁለት\n",
            "ልጆቿን\n",
            "ወደ\n",
            "ደቡብ\n",
            "አፍሪካ\n",
            "ላከች፡፡6ኛው\n",
            "ቢግ\n",
            "ብራዘርስ\n",
            "አፍሪካ\n",
            "አብሮ\n",
            "የመኖር\n",
            "ውድድር\n",
            "በደቡብ\n",
            "አፍሪካ\n",
            "ተካሂዷል፡፡\n",
            "ከተለያዩ\n",
            "14\n",
            "የአፍሪካ\n",
            "አገራት\n",
            "የተውጣጡ\n",
            "26\n",
            "ያህል\n",
            "ተሳታፊዎች\n",
            "የተካፈሉበት\n",
            "ይህ\n",
            "ውድድር፣\n",
            "ግለሰቦች\n",
            "በፈታኝ\n",
            "ሁኔታ\n",
            "ውስጥ\n",
            "በማለፍ\n",
            "ብቃታቸውን\n",
            "የሚያስመሰክሩበት\n",
            "መሆኑን\n",
            "ሰምተናል፡፡\n",
            "የሚገጥሟቸውን\n",
            "የተለያዩ\n",
            "ፈተናዎች\n",
            "በትእግስትና\n",
            "በጥበብ\n",
            "ማለፍ፣\n",
            "ከሌሎች\n",
            "ጋር\n",
            "ተስማምቶ\n",
            "መዝለቅ፣\n",
            "ችግሮችን\n",
            "በብልጠት\n",
            "መፍታት\n",
            "ወዘተ\n",
            "በየጊዜው\n",
            "ከሚደረገው\n",
            "ቅነሳ\n",
            "ተርፈው\n",
            "ለ91\n",
            "ቀናት\n",
            "ያህል\n",
            "በውድድሩ\n",
            "መቆየት\n",
            "የቻሉ\n",
            "ሁለት\n",
            "ተወዳዳሪዎች\n",
            "እያንዳንዳቸው\n",
            "200\n",
            "ሺህ\n",
            "ዶላር\n",
            "እንደሚ\n",
            "\n",
            "2-grams:\n",
            "ምን መሰላችሁ?\n",
            "መሰላችሁ? (አንባቢያን)\n",
            "(አንባቢያን) ኢትዮጵያ\n",
            "ኢትዮጵያ በተደጋጋሚ\n",
            "በተደጋጋሚ ጥሪው\n",
            "ጥሪው ደርሷት\n",
            "ደርሷት ልትታደመው\n",
            "ልትታደመው ያልቻለችው\n",
            "ያልቻለችው የአለም\n",
            "የአለም የእግር\n",
            "የእግር ኳስ\n",
            "ኳስ ዋ\n",
            "ዋ ለ19ኛ\n",
            "ለ19ኛ ጊዜ\n",
            "ጊዜ በደቡብ\n",
            "በደቡብ አፍሪካ\n",
            "አፍሪካ ሲጠጣ፣\n",
            "ሲጠጣ፣ በሩቅ\n",
            "በሩቅ እያየች\n",
            "እያየች አንጀቷ\n",
            "አንጀቷ ባረረ\n",
            "ባረረ ልክ\n",
            "ልክ በአመቱ\n",
            "በአመቱ በለስ\n",
            "በለስ ቀናትና\n",
            "ቀናትና ሌላ\n",
            "ሌላ ዋ\n",
            "ዋ ልትታደም\n",
            "ልትታደም ሁለት\n",
            "ሁለት ልጆቿን\n",
            "ልጆቿን ወደ\n",
            "ወደ ደቡብ\n",
            "ደቡብ አፍሪካ\n",
            "አፍሪካ ላከች፡፡6ኛው\n",
            "ላከች፡፡6ኛው ቢግ\n",
            "ቢግ ብራዘርስ\n",
            "ብራዘርስ አፍሪካ\n",
            "አፍሪካ አብሮ\n",
            "አብሮ የመኖር\n",
            "የመኖር ውድድር\n",
            "ውድድር በደቡብ\n",
            "በደቡብ አፍሪካ\n",
            "አፍሪካ ተካሂዷል፡፡\n",
            "ተካሂዷል፡፡ ከተለያዩ\n",
            "ከተለያዩ 14\n",
            "14 የአፍሪካ\n",
            "የአፍሪካ አገራት\n",
            "አገራት የተውጣጡ\n",
            "የተውጣጡ 26\n",
            "26 ያህል\n",
            "ያህል ተሳታፊዎች\n",
            "ተሳታፊዎች የተካፈሉበት\n",
            "የተካፈሉበት ይህ\n",
            "ይህ ውድድር፣\n",
            "ውድድር፣ ግለሰቦች\n",
            "ግለሰቦች በፈታኝ\n",
            "በፈታኝ ሁኔታ\n",
            "ሁኔታ ውስጥ\n",
            "ውስጥ በማለፍ\n",
            "በማለፍ ብቃታቸውን\n",
            "ብቃታቸውን የሚያስመሰክሩበት\n",
            "የሚያስመሰክሩበት መሆኑን\n",
            "መሆኑን ሰምተናል፡፡\n",
            "ሰምተናል፡፡ የሚገጥሟቸውን\n",
            "የሚገጥሟቸውን የተለያዩ\n",
            "የተለያዩ ፈተናዎች\n",
            "ፈተናዎች በትእግስትና\n",
            "በትእግስትና በጥበብ\n",
            "በጥበብ ማለፍ፣\n",
            "ማለፍ፣ ከሌሎች\n",
            "ከሌሎች ጋር\n",
            "ጋር ተስማምቶ\n",
            "ተስማምቶ መዝለቅ፣\n",
            "መዝለቅ፣ ችግሮችን\n",
            "ችግሮችን በብልጠት\n",
            "በብልጠት መፍታት\n",
            "መፍታት ወዘተ\n",
            "ወዘተ በየጊዜው\n",
            "በየጊዜው ከሚደረገው\n",
            "ከሚደረገው ቅነሳ\n",
            "ቅነሳ ተርፈው\n",
            "ተርፈው ለ91\n",
            "ለ91 ቀናት\n",
            "ቀናት ያህል\n",
            "ያህል በውድድሩ\n",
            "በውድድሩ መቆየት\n",
            "መቆየት የቻሉ\n",
            "የቻሉ ሁለት\n",
            "ሁለት ተወዳዳሪዎች\n",
            "ተወዳዳሪዎች እያንዳንዳቸው\n",
            "እያንዳንዳቸው 200\n",
            "200 ሺህ\n",
            "ሺህ ዶላር\n",
            "ዶላር እንደሚ\n",
            "\n",
            "3-grams:\n",
            "ምን መሰላችሁ? (አንባቢያን)\n",
            "መሰላችሁ? (አንባቢያን) ኢትዮጵያ\n",
            "(አንባቢያን) ኢትዮጵያ በተደጋጋሚ\n",
            "ኢትዮጵያ በተደጋጋሚ ጥሪው\n",
            "በተደጋጋሚ ጥሪው ደርሷት\n",
            "ጥሪው ደርሷት ልትታደመው\n",
            "ደርሷት ልትታደመው ያልቻለችው\n",
            "ልትታደመው ያልቻለችው የአለም\n",
            "ያልቻለችው የአለም የእግር\n",
            "የአለም የእግር ኳስ\n",
            "የእግር ኳስ ዋ\n",
            "ኳስ ዋ ለ19ኛ\n",
            "ዋ ለ19ኛ ጊዜ\n",
            "ለ19ኛ ጊዜ በደቡብ\n",
            "ጊዜ በደቡብ አፍሪካ\n",
            "በደቡብ አፍሪካ ሲጠጣ፣\n",
            "አፍሪካ ሲጠጣ፣ በሩቅ\n",
            "ሲጠጣ፣ በሩቅ እያየች\n",
            "በሩቅ እያየች አንጀቷ\n",
            "እያየች አንጀቷ ባረረ\n",
            "አንጀቷ ባረረ ልክ\n",
            "ባረረ ልክ በአመቱ\n",
            "ልክ በአመቱ በለስ\n",
            "በአመቱ በለስ ቀናትና\n",
            "በለስ ቀናትና ሌላ\n",
            "ቀናትና ሌላ ዋ\n",
            "ሌላ ዋ ልትታደም\n",
            "ዋ ልትታደም ሁለት\n",
            "ልትታደም ሁለት ልጆቿን\n",
            "ሁለት ልጆቿን ወደ\n",
            "ልጆቿን ወደ ደቡብ\n",
            "ወደ ደቡብ አፍሪካ\n",
            "ደቡብ አፍሪካ ላከች፡፡6ኛው\n",
            "አፍሪካ ላከች፡፡6ኛው ቢግ\n",
            "ላከች፡፡6ኛው ቢግ ብራዘርስ\n",
            "ቢግ ብራዘርስ አፍሪካ\n",
            "ብራዘርስ አፍሪካ አብሮ\n",
            "አፍሪካ አብሮ የመኖር\n",
            "አብሮ የመኖር ውድድር\n",
            "የመኖር ውድድር በደቡብ\n",
            "ውድድር በደቡብ አፍሪካ\n",
            "በደቡብ አፍሪካ ተካሂዷል፡፡\n",
            "አፍሪካ ተካሂዷል፡፡ ከተለያዩ\n",
            "ተካሂዷል፡፡ ከተለያዩ 14\n",
            "ከተለያዩ 14 የአፍሪካ\n",
            "14 የአፍሪካ አገራት\n",
            "የአፍሪካ አገራት የተውጣጡ\n",
            "አገራት የተውጣጡ 26\n",
            "የተውጣጡ 26 ያህል\n",
            "26 ያህል ተሳታፊዎች\n",
            "ያህል ተሳታፊዎች የተካፈሉበት\n",
            "ተሳታፊዎች የተካፈሉበት ይህ\n",
            "የተካፈሉበት ይህ ውድድር፣\n",
            "ይህ ውድድር፣ ግለሰቦች\n",
            "ውድድር፣ ግለሰቦች በፈታኝ\n",
            "ግለሰቦች በፈታኝ ሁኔታ\n",
            "በፈታኝ ሁኔታ ውስጥ\n",
            "ሁኔታ ውስጥ በማለፍ\n",
            "ውስጥ በማለፍ ብቃታቸውን\n",
            "በማለፍ ብቃታቸውን የሚያስመሰክሩበት\n",
            "ብቃታቸውን የሚያስመሰክሩበት መሆኑን\n",
            "የሚያስመሰክሩበት መሆኑን ሰምተናል፡፡\n",
            "መሆኑን ሰምተናል፡፡ የሚገጥሟቸውን\n",
            "ሰምተናል፡፡ የሚገጥሟቸውን የተለያዩ\n",
            "የሚገጥሟቸውን የተለያዩ ፈተናዎች\n",
            "የተለያዩ ፈተናዎች በትእግስትና\n",
            "ፈተናዎች በትእግስትና በጥበብ\n",
            "በትእግስትና በጥበብ ማለፍ፣\n",
            "በጥበብ ማለፍ፣ ከሌሎች\n",
            "ማለፍ፣ ከሌሎች ጋር\n",
            "ከሌሎች ጋር ተስማምቶ\n",
            "ጋር ተስማምቶ መዝለቅ፣\n",
            "ተስማምቶ መዝለቅ፣ ችግሮችን\n",
            "መዝለቅ፣ ችግሮችን በብልጠት\n",
            "ችግሮችን በብልጠት መፍታት\n",
            "በብልጠት መፍታት ወዘተ\n",
            "መፍታት ወዘተ በየጊዜው\n",
            "ወዘተ በየጊዜው ከሚደረገው\n",
            "በየጊዜው ከሚደረገው ቅነሳ\n",
            "ከሚደረገው ቅነሳ ተርፈው\n",
            "ቅነሳ ተርፈው ለ91\n",
            "ተርፈው ለ91 ቀናት\n",
            "ለ91 ቀናት ያህል\n",
            "ቀናት ያህል በውድድሩ\n",
            "ያህል በውድድሩ መቆየት\n",
            "በውድድሩ መቆየት የቻሉ\n",
            "መቆየት የቻሉ ሁለት\n",
            "የቻሉ ሁለት ተወዳዳሪዎች\n",
            "ሁለት ተወዳዳሪዎች እያንዳንዳቸው\n",
            "ተወዳዳሪዎች እያንዳንዳቸው 200\n",
            "እያንዳንዳቸው 200 ሺህ\n",
            "200 ሺህ ዶላር\n",
            "ሺህ ዶላር እንደሚ\n",
            "\n",
            "4-grams:\n",
            "ምን መሰላችሁ? (አንባቢያን) ኢትዮጵያ\n",
            "መሰላችሁ? (አንባቢያን) ኢትዮጵያ በተደጋጋሚ\n",
            "(አንባቢያን) ኢትዮጵያ በተደጋጋሚ ጥሪው\n",
            "ኢትዮጵያ በተደጋጋሚ ጥሪው ደርሷት\n",
            "በተደጋጋሚ ጥሪው ደርሷት ልትታደመው\n",
            "ጥሪው ደርሷት ልትታደመው ያልቻለችው\n",
            "ደርሷት ልትታደመው ያልቻለችው የአለም\n",
            "ልትታደመው ያልቻለችው የአለም የእግር\n",
            "ያልቻለችው የአለም የእግር ኳስ\n",
            "የአለም የእግር ኳስ ዋ\n",
            "የእግር ኳስ ዋ ለ19ኛ\n",
            "ኳስ ዋ ለ19ኛ ጊዜ\n",
            "ዋ ለ19ኛ ጊዜ በደቡብ\n",
            "ለ19ኛ ጊዜ በደቡብ አፍሪካ\n",
            "ጊዜ በደቡብ አፍሪካ ሲጠጣ፣\n",
            "በደቡብ አፍሪካ ሲጠጣ፣ በሩቅ\n",
            "አፍሪካ ሲጠጣ፣ በሩቅ እያየች\n",
            "ሲጠጣ፣ በሩቅ እያየች አንጀቷ\n",
            "በሩቅ እያየች አንጀቷ ባረረ\n",
            "እያየች አንጀቷ ባረረ ልክ\n",
            "አንጀቷ ባረረ ልክ በአመቱ\n",
            "ባረረ ልክ በአመቱ በለስ\n",
            "ልክ በአመቱ በለስ ቀናትና\n",
            "በአመቱ በለስ ቀናትና ሌላ\n",
            "በለስ ቀናትና ሌላ ዋ\n",
            "ቀናትና ሌላ ዋ ልትታደም\n",
            "ሌላ ዋ ልትታደም ሁለት\n",
            "ዋ ልትታደም ሁለት ልጆቿን\n",
            "ልትታደም ሁለት ልጆቿን ወደ\n",
            "ሁለት ልጆቿን ወደ ደቡብ\n",
            "ልጆቿን ወደ ደቡብ አፍሪካ\n",
            "ወደ ደቡብ አፍሪካ ላከች፡፡6ኛው\n",
            "ደቡብ አፍሪካ ላከች፡፡6ኛው ቢግ\n",
            "አፍሪካ ላከች፡፡6ኛው ቢግ ብራዘርስ\n",
            "ላከች፡፡6ኛው ቢግ ብራዘርስ አፍሪካ\n",
            "ቢግ ብራዘርስ አፍሪካ አብሮ\n",
            "ብራዘርስ አፍሪካ አብሮ የመኖር\n",
            "አፍሪካ አብሮ የመኖር ውድድር\n",
            "አብሮ የመኖር ውድድር በደቡብ\n",
            "የመኖር ውድድር በደቡብ አፍሪካ\n",
            "ውድድር በደቡብ አፍሪካ ተካሂዷል፡፡\n",
            "በደቡብ አፍሪካ ተካሂዷል፡፡ ከተለያዩ\n",
            "አፍሪካ ተካሂዷል፡፡ ከተለያዩ 14\n",
            "ተካሂዷል፡፡ ከተለያዩ 14 የአፍሪካ\n",
            "ከተለያዩ 14 የአፍሪካ አገራት\n",
            "14 የአፍሪካ አገራት የተውጣጡ\n",
            "የአፍሪካ አገራት የተውጣጡ 26\n",
            "አገራት የተውጣጡ 26 ያህል\n",
            "የተውጣጡ 26 ያህል ተሳታፊዎች\n",
            "26 ያህል ተሳታፊዎች የተካፈሉበት\n",
            "ያህል ተሳታፊዎች የተካፈሉበት ይህ\n",
            "ተሳታፊዎች የተካፈሉበት ይህ ውድድር፣\n",
            "የተካፈሉበት ይህ ውድድር፣ ግለሰቦች\n",
            "ይህ ውድድር፣ ግለሰቦች በፈታኝ\n",
            "ውድድር፣ ግለሰቦች በፈታኝ ሁኔታ\n",
            "ግለሰቦች በፈታኝ ሁኔታ ውስጥ\n",
            "በፈታኝ ሁኔታ ውስጥ በማለፍ\n",
            "ሁኔታ ውስጥ በማለፍ ብቃታቸውን\n",
            "ውስጥ በማለፍ ብቃታቸውን የሚያስመሰክሩበት\n",
            "በማለፍ ብቃታቸውን የሚያስመሰክሩበት መሆኑን\n",
            "ብቃታቸውን የሚያስመሰክሩበት መሆኑን ሰምተናል፡፡\n",
            "የሚያስመሰክሩበት መሆኑን ሰምተናል፡፡ የሚገጥሟቸውን\n",
            "መሆኑን ሰምተናል፡፡ የሚገጥሟቸውን የተለያዩ\n",
            "ሰምተናል፡፡ የሚገጥሟቸውን የተለያዩ ፈተናዎች\n",
            "የሚገጥሟቸውን የተለያዩ ፈተናዎች በትእግስትና\n",
            "የተለያዩ ፈተናዎች በትእግስትና በጥበብ\n",
            "ፈተናዎች በትእግስትና በጥበብ ማለፍ፣\n",
            "በትእግስትና በጥበብ ማለፍ፣ ከሌሎች\n",
            "በጥበብ ማለፍ፣ ከሌሎች ጋር\n",
            "ማለፍ፣ ከሌሎች ጋር ተስማምቶ\n",
            "ከሌሎች ጋር ተስማምቶ መዝለቅ፣\n",
            "ጋር ተስማምቶ መዝለቅ፣ ችግሮችን\n",
            "ተስማምቶ መዝለቅ፣ ችግሮችን በብልጠት\n",
            "መዝለቅ፣ ችግሮችን በብልጠት መፍታት\n",
            "ችግሮችን በብልጠት መፍታት ወዘተ\n",
            "በብልጠት መፍታት ወዘተ በየጊዜው\n",
            "መፍታት ወዘተ በየጊዜው ከሚደረገው\n",
            "ወዘተ በየጊዜው ከሚደረገው ቅነሳ\n",
            "በየጊዜው ከሚደረገው ቅነሳ ተርፈው\n",
            "ከሚደረገው ቅነሳ ተርፈው ለ91\n",
            "ቅነሳ ተርፈው ለ91 ቀናት\n",
            "ተርፈው ለ91 ቀናት ያህል\n",
            "ለ91 ቀናት ያህል በውድድሩ\n",
            "ቀናት ያህል በውድድሩ መቆየት\n",
            "ያህል በውድድሩ መቆየት የቻሉ\n",
            "በውድድሩ መቆየት የቻሉ ሁለት\n",
            "መቆየት የቻሉ ሁለት ተወዳዳሪዎች\n",
            "የቻሉ ሁለት ተወዳዳሪዎች እያንዳንዳቸው\n",
            "ሁለት ተወዳዳሪዎች እያንዳንዳቸው 200\n",
            "ተወዳዳሪዎች እያንዳንዳቸው 200 ሺህ\n",
            "እያንዳንዳቸው 200 ሺህ ዶላር\n",
            "200 ሺህ ዶላር እንደሚ\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.2 Calculate probabilities of n-grams and find the top 10 most likely n-grams for all n.\n"
      ],
      "metadata": {
        "id": "QhRLIBM7LFrS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate and print n-grams for different values of n\n",
        "for n in [1, 2, 3, 4]:\n",
        "    ngrams = generate_ngrams(words, n)\n",
        "\n",
        "    # Calculate probabilities\n",
        "    probabilities = calculate_ngram_probabilities(ngrams)\n",
        "\n",
        "    # Find the top 10 most likely n-grams\n",
        "    top_10_ngrams = sorted(probabilities.items(), key=lambda x: x[1], reverse=True)[:10]\n",
        "\n",
        "    # Print results\n",
        "    print(f'{n}-grams:')\n",
        "    print(\"Top 10 most likely:\")\n",
        "    print()\n",
        "    for ngram, probability in top_10_ngrams:\n",
        "        print(f\"{ngram}: {probability:.4f}\")\n",
        "\n",
        "    print()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o9XozYDVgaHt",
        "outputId": "ae148213-9627-497c-8b1c-0273e4f66124"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1-grams:\n",
            "Top 10 most likely:\n",
            "\n",
            "ላይ: 0.0090\n",
            "ነው፡፡: 0.0079\n",
            "ነው: 0.0069\n",
            "እና: 0.0045\n",
            "ግን: 0.0045\n",
            "ወደ: 0.0045\n",
            "ውስጥ: 0.0044\n",
            "ጋር: 0.0037\n",
            "ነገር: 0.0036\n",
            "ጊዜ: 0.0031\n",
            "\n",
            "2-grams:\n",
            "Top 10 most likely:\n",
            "\n",
            "ዓ ም: 0.0011\n",
            "( ): 0.0008\n",
            "ነገር ግን: 0.0006\n",
            "ብቻ ሳይሆን: 0.0004\n",
            "አዲስ አበባ: 0.0004\n",
            "ማለት ነው፡፡: 0.0003\n",
            "ምን ያህል: 0.0003\n",
            "በአዲስ አበባ: 0.0003\n",
            "እግር ኳስ: 0.0002\n",
            "ሚሊዮን ዶላር: 0.0002\n",
            "\n",
            "3-grams:\n",
            "Top 10 most likely:\n",
            "\n",
            "እ ኤ አ: 0.0002\n",
            "የኢትዮጵያ ብሄራዊ ቡድን: 0.0001\n",
            "2004 ዓ ም: 0.0001\n",
            "2005 ዓ ም: 0.0001\n",
            "ወደ አዲስ አበባ: 0.0001\n",
            "2006 ዓ ም: 0.0001\n",
            "ቂ ቂ ቂ: 0.0001\n",
            "ዓ ም ጀምሮ: 0.0001\n",
            "ነው፡፡ ነገር ግን: 0.0001\n",
            "ቀን 2006 ዓ: 0.0000\n",
            "\n",
            "4-grams:\n",
            "Top 10 most likely:\n",
            "\n",
            "ቀን 2005 ዓ ም: 0.0000\n",
            "ቀን 2006 ዓ ም: 0.0000\n",
            "ጨዋታን ጨዋታ ያነሳው የለ: 0.0000\n",
            "ቀን 2004 ዓ ም: 0.0000\n",
            "የጽንስና ማህጸን ሐኪሞች ማህበር: 0.0000\n",
            "እንግዲህ ጨዋታን ጨዋታ ያነሳው: 0.0000\n",
            "የኢትዮጵያ የጽንስና ማህጸን ሐኪሞች: 0.0000\n",
            "ጠቅላይ ሚኒስትር መለስ ዜናዊ: 0.0000\n",
            "የኢትዮጵያ እግር ኳስ ፌደሬሽን: 0.0000\n",
            "ዋና አሰልጣኝ ሰውነት ቢሻው: 0.0000\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.3 What is the probability of the sentence. \"ኢትዮጵያ ታሪካዊ ሀገር ናት \". You can also try more sentences.\n"
      ],
      "metadata": {
        "id": "mszRqoukpBaw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_sentence_probability(sentence, ngram_probabilities):\n",
        "    sentence_ngrams = generate_ngrams(sentence.split(), len(list(ngram_probabilities.keys())[0].split()))\n",
        "    sentence_probability = 1.0\n",
        "    for ngram in sentence_ngrams:\n",
        "        sentence_probability *= ngram_probabilities.get(ngram, 0.0)\n",
        "    return sentence_probability\n",
        "\n",
        "# Calculate n-gram probabilities for a sample\n",
        "ngrams = generate_ngrams(words, 4)\n",
        "ngram_probabilities = calculate_ngram_probabilities(ngrams)\n",
        "\n",
        "sentences = [\"ኢትዮጵያ ታሪካዊ ሀገር ናት\", \"ኢትዮጵያ በተደጋጋሚ ጥሪው ደርሷት\", \"ልትታደመው ያልቻለችው የአለም የእግር ኳስ\", \"በአመቱ በለስ ቀናትና ሌላ\", \"ለእናንተም መልካም ቀን\"]\n",
        "\n",
        "# Calculate the probability of the sentences\n",
        "for sentence in sentences:\n",
        "  sentence_probability = calculate_sentence_probability(sentence, ngram_probabilities)\n",
        "  print(f\"The probability of the sentence '{sentence}' is: {sentence_probability:.10f}\")\n",
        "print()\n"
      ],
      "metadata": {
        "id": "EqkHuiODV5ce",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f5c0ea58-ed60-40f5-f213-3b92a63ab0f6"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The probability of the sentence 'ኢትዮጵያ ታሪካዊ ሀገር ናት' is: 0.0000000000\n",
            "The probability of the sentence 'ኢትዮጵያ በተደጋጋሚ ጥሪው ደርሷት' is: 0.0000003556\n",
            "The probability of the sentence 'ልትታደመው ያልቻለችው የአለም የእግር ኳስ' is: 0.0000000000\n",
            "The probability of the sentence 'በአመቱ በለስ ቀናትና ሌላ' is: 0.0000003556\n",
            "The probability of the sentence 'ለእናንተም መልካም ቀን' is: 1.0000000000\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.4 Generate random sentences using n-grams; explain what happens as n increases, based on your output.\n"
      ],
      "metadata": {
        "id": "tRUAp5GWpECv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "def generate_random_sentence(ngram_probabilities, max_length=20):\n",
        "    sentence = []\n",
        "    current_ngram = random.choice(list(ngram_probabilities.keys()))\n",
        "\n",
        "    while len(sentence) < max_length:\n",
        "        sentence.extend(current_ngram.split())\n",
        "        next_word = random.choices(\n",
        "            list(ngram_probabilities.keys()),\n",
        "            weights=list(ngram_probabilities.values())\n",
        "        )[0].split()[-1]\n",
        "        current_ngram = ' '.join(current_ngram.split()[1:] + [next_word])\n",
        "\n",
        "    return ' '.join(sentence)\n",
        "\n",
        "# Generate random sentences for different values of n\n",
        "for n in [1, 2, 3, 4]:\n",
        "    ngrams = generate_ngrams(words, n)\n",
        "    ngram_probabilities = calculate_ngram_probabilities(ngrams)\n",
        "    print(f\"\\nGenerated random sentence for n: {n}:\")\n",
        "    random_sentence = generate_random_sentence(ngram_probabilities, 20)\n",
        "    print(random_sentence)\n"
      ],
      "metadata": {
        "id": "pkTXXxDcWa9B",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5d4f11fd-23f1-4057-ec60-425e45f3bfa6"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Generated random sentence for n: 1:\n",
            "አጠረዉ፤ ዘይቱን የምናውቀው የከፋ ኬሺ ነበር ነው ኮሚቴ ብቻ እንዲሁም የተሰኘ ብዙዎችን ነበር፡፡በዓለም ብለዋል፡፡ ተቀባይነት የሚናገሩት ላይ ላመጡኝ አሰኘ፡፡ ሆነ\n",
            "\n",
            "Generated random sentence for n: 2:\n",
            "ብዙም ኢምፓየር ኢምፓየር የተለቀቁት የተለቀቁት እጅጉና እጅጉና በማትፈልገኝ በማትፈልገኝ ከጤነኛ ከጤነኛ ታቦት ታቦት ለማካሄድ ለማካሄድ አስር አስር የሚማታበት የሚማታበት የህዝብ\n",
            "\n",
            "Generated random sentence for n: 3:\n",
            "ቅርንጫፎች አሉት ሞባይልና አሉት ሞባይልና ሲመላለስ፣ ሞባይልና ሲመላለስ፣ ቆጥሮ ሲመላለስ፣ ቆጥሮ ብሎኝ ቆጥሮ ብሎኝ ሆረስ ብሎኝ ሆረስ የመርገጥና ሆረስ የመርገጥና ጀምሮ\n",
            "\n",
            "Generated random sentence for n: 4:\n",
            "በተገቢው ቦታ መነሳቱ ምንም ቦታ መነሳቱ ምንም እንደሌለውም መነሳቱ ምንም እንደሌለውም ግን ምንም እንደሌለውም ግን ማለቱን እንደሌለውም ግን ማለቱን ነቅናቂ\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Explanation:\n",
        "\n",
        "As the value of n increases:\n",
        "\n",
        "1. **Increased Coherence:** Longer n-grams capture more context, making generated sentences more coherent.\n",
        "\n",
        "2. **Higher Repetition Risk:** Sensitivity to specific patterns in training data may lead to repetitive sequences in generated sentences.\n",
        "\n",
        "3. **Balancing Act:** Choosing an appropriate n involves a trade-off between capturing context and avoiding overfitting to training data.\n",
        "\n",
        "In summary, larger n improves coherence but increases the risk of repetition, requiring a careful balance based on the specific characteristics of the language model and training data."
      ],
      "metadata": {
        "id": "8N1H_gjwtG9L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# #2 Evaluate these Language Models Using Intrinsic Evaluation Method (4%)\n"
      ],
      "metadata": {
        "id": "lDKrkt4RqiW5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Downloading necessary packages"
      ],
      "metadata": {
        "id": "JKeVtDpuzCjM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.lm import MLE\n",
        "from nltk.lm.preprocessing import padded_everygram_pipeline\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from nltk.lm.vocabulary import Vocabulary\n",
        "\n",
        "# Download the necessary NLTK resources\n",
        "nltk.download('punkt')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IzN5g_-Lw0W3",
        "outputId": "d6c16c3c-6c1d-4ba3-85fc-864088dd985b"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Pre-Processing\n"
      ],
      "metadata": {
        "id": "_eYu2GshypHR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize the text into sentences and words\n",
        "tokenized_text = [word_tokenize(sent) for sent in sent_tokenize(text)]\n",
        "\n",
        "# Split the data into training and validation sets\n",
        "train_data, valid_data = tokenized_text[:int(0.8 * len(tokenized_text))], tokenized_text[int(0.8 * len(tokenized_text)):]"
      ],
      "metadata": {
        "id": "p0Q32F0zyn0y"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training a Maximum Likelihood Estimation (MLE) language model\n"
      ],
      "metadata": {
        "id": "1vD3HbmBzs3c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n = 4\n",
        "train_ngrams, vocab = padded_everygram_pipeline(n, train_data)\n",
        "\n",
        "# Convert vocab to nltk.lm.Vocabulary\n",
        "vocab = Vocabulary(vocab)\n",
        "\n",
        "lm = MLE(order=n, vocabulary=vocab)\n",
        "lm.fit(train_ngrams)\n",
        "\n",
        "print(\"Training Finished\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nw0blFcYWosn",
        "outputId": "55514389-58d7-41d1-c690-028175312a0e"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Finished\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Intrinsic Evaluation Metrics"
      ],
      "metadata": {
        "id": "dXOZrFc90es5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Perplexity\n",
        "valid_ngrams = [list(nltk.lm.preprocessing.pad_both_ends(sent, n=n)) for sent in valid_data]\n",
        "perplexity = lm.perplexity(valid_ngrams)\n",
        "\n",
        "# 2. Grammatical Correctness\n",
        "generated_sentence = random_sentence\n",
        "generated_sentence_tokens = word_tokenize(generated_sentence)\n",
        "grammatical_correctness = all(token in vocab for token in generated_sentence_tokens)\n",
        "\n",
        "# Print Evaluation Metrics\n",
        "print(f\"Perplexity: {perplexity:.4f}\")\n",
        "print(f\"Grammatical Correctness: {grammatical_correctness}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8VB4gLzSydtH",
        "outputId": "374b39f2-05a0-4d15-8815-c366522750df"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Perplexity: inf\n",
            "Grammatical Correctness: True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# #3 Evaluate these Language Models Using Extrinsic Evaluation Method (4%)"
      ],
      "metadata": {
        "id": "y1oNqL3U5AXM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Extrinsic Evaluation: Text Completion Task\n"
      ],
      "metadata": {
        "id": "zMPyNcHV45OL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def complete_text(input_text, model, n):\n",
        "    input_tokens = nltk.word_tokenize(input_text)\n",
        "    input_ngrams = list(nltk.ngrams(input_tokens, n))\n",
        "\n",
        "    # Predict the next word using the language model\n",
        "    next_word = model.generate(1)\n",
        "\n",
        "    return input_text + ' ' + next_word\n",
        "\n",
        "# Example usage\n",
        "input_text = \"የአፍሪካ አገራት\"\n",
        "completed_text = complete_text(input_text, lm, 4)\n",
        "print(\"Completed Text:\", completed_text)\n"
      ],
      "metadata": {
        "id": "EJ3ThOLwY_Lo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "233a98e1-4b04-4616-f201-dc298b7f0a09"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Completed Text: የአፍሪካ አገራት የሚከተለው\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-m_aWW7mBOuc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}